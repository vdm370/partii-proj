\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{verbatim}
\usepackage[ampersand]{easylist}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{amsbsy}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{color} 
\usepackage{cleveref}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{etex}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{footnote}
\usepackage{framed}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{isodate}
\usepackage{latexsym}
\usepackage{leftidx}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage{soul}
\usepackage{subfig}
\usepackage{tikz}
\usepackage[nottoc]{tocbibind}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\input{epsf}                            % to allow postscript inclusions
% On thor and CUS read top of file:
%     /opt/TeX/lib/texmf/tex/dvips/epsf.sty
% On CL machines read:
%     /usr/lib/tex/macros/dvips/epsf.tex



\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Vladimir Milenkovi\'c}

\vspace*{60mm}
\begin{center}
\Huge
{\bf The Travelling Salesman problem - Comparison of different approaches } \\
\vspace*{5mm}
Diploma in Computer Science \\
\vspace*{5mm}
Trinity College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Vladimir Milenkovi\'c                  \\
College:            & \bf Trinity College                    \\
Project Title:      & \bf The Travelling Salesman problem - \\
& \bf Comparison of different approaches \\
Examination:        & \bf Diploma in Computer Science, May 2020        \\
Word Count:         & \bf TODO\footnotemark[1] \\
Project Originator: & \bf Dr T. Sauerwald and V. Milenkovic                    \\
Supervisor:         & \bf Dr T. Sauerwald                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by {\tt detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

To code, compare and contrast different algorithms and approaches in solving one of the most famous problems ever to exist - the Travelling Salesman Problem. All the code done for this project is easy to use for anybody wishing to tackle this problem, and the overview of all the algorithms with the suggested usecase for each one is included as well.


\section*{Work Completed}

All work that was done during this project will be mentioned in the dissertation, the code will be submitted as well.

\section*{Special Difficulties}

Getting Lin-Kernighan algorithm to work significantly better than the other algorithms, reproducing the results mentioned on the original paper. Also, the Blossom algorithm for minimum weight perfect matching is hard to get working. % TODO
 
\newpage
\section*{Declaration}

I, Vladimir Milenkovic of Trinity College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed Vladimir Milenkovic}

\medskip
\leftline{Date} % TODO

\cleardoublepage

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Add some acknowledgements here. Please, do add some acknowledgements here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

\chapter{Introduction}

\section{Motivation}

My project tries to tackle one of the most famous problems in the history of Computer science - the Travelling Salesman problem. In that problem, we are given a weighted graph $G = (V, E, w)$, where $V$ denotes the set of vertices, $E$ denotes a set of edges, and $w : E \rightarrow \mathbb{R^{+}}$ is a weight function, mapping edges to weight that's assigned to them. We are interesting in finding a minimum-weight cycle in this graph. As we can see, a problem similar to this can arise in many real-world situations (a mailman having to visit a list of houses to deliver the mail, for example), so it is expected that a lot of effort has been put into solving this problem as efficiently as possible. This problem is known to be $\mathcal{NP}$-hard, basically meaning that there is no polynomial time algorithm solving this problem. With that in mind, we are forced to release some requirements of our algorithm: not demanding that we get a fully correct solution, only sufficiently close to the solution, and/or not demanding to finish in polynomial time, but sufficient fast. This project will try to give a summary of some more or less successful attemps, and analyze what are the advangates and disadvantages of the algorithms involved.

\section{Algorithms}

In this section, I will list all the algorithms that I have taken into consideration for this project. I have made this choice after analyzing already existings benchhmarks about Travelling Salesman problem, trying to find the best tradeoff between complexity, running time, difficulty to reproduce and the result that the algorithm is producing. After having that finished, I have decided to split the algorithms in several classes, listed below:

\begin{itemize}

\item {\bf Exact algorithms}
\item {\bf Approximation algorithms}
\item {\bf Improvement algorithms}
\item {\bf Heuristic algorithms}
\item {\bf Optimization algorithms} 
\end{itemize}

\subsection{Exact algorithms}

{\bf Exact algorithms} are those algorithms which produce the correct solution every time they are run. Knowing the $\mathcal{NP}$-hardness of the TSP, we know that there is no algorithm of this kind running in polynomial time. I have considered 3 algorithms here:

\begin{itemize}

\item {\it Brute-force algorithm}
\item {\it Held-Karp (dynamic programming) algorithm}
\item {\it Branch-and-Bound algorithm}

\end{itemize}

\subsection{Approximation algorithms}

{\bf Approximation algorithms} are ones that can guarantee some bound concerning the output of the algorithm. For example, the {\it 2-approximation algorithm} guarantees that the cost of the cycle it returned is at most two times larger than the cost of the optimal cycle. They usually run in polynomial time, and all the algorithms I've included in the project do belong in that category.

\begin{itemize}

\item {\it 2-approximation algorithm}
\item {\it Christofides-greedy algorithm}
\item {\it Christofides algorithm}

\end{itemize}

\subsection{Improvement algorithms}

{\bf Improvement algorithms} are those algorithm which consecutively try to improve an existing solution. Starting with some solution (greedy, for example), they try to alter it in a way that a better solution is produced. Running this algorithm for some time will eventually make the algorithm converge, getting a solution which we take as a final one. In practice, combining improvement techniques with some good strategies for the initial solution produce the best results, and a lot of best solutions on well-known TSP instances are obtained using improvement methods.

\begin{itemize}

\item {\it 2-opt algorithm}
\item {\it 3-opt algorithm}
% TODO \item {\it 4-opt algorithm}
\item {\it Lin-Kernighan algorithm}

\end{itemize}

\subsection{Heuristic algorithms}

{\bf Heuristic algorithms} are algorithms which usually have polynomial running time, usually do not have a guaranteed bound of error, but in general they produce results that are close to the optimal solution. There are a lot of different heuristic for tackling this problem, so I have selected a few that did produce some solid results. Also, these algorithms should not be too hard to code or to understand, in general, and it is surprising how good results can they achieve:

\begin{itemize}

\item {\it Random algorithm}
\item {\it Nearest neighbour algorithm (greedy)}
\item {\it Insertion heuristic - cheapest, farthest, nearest, random}
\item {\it Convex-hull heuristic algorithm}

\end{itemize}

\subsection{Optimization algorithms}

In {\bf optimization algorithms}, we are starting with random solutions, computing the results those solutions are producing and then adapting towards solutions which are more preferable (have less cost in this example). Running this iteratively, we converge to a solution which can be pretty close to the correct one, no guarantees shown. There are also quite some algorithms which perform the process outlined above, here are some:

\begin{itemize}

\item {\it Ant-colony optimization algorithm}
\item {\it Simulated annealing algorithm}

\end{itemize}

\cleardoublepage



\chapter{Preparation}

In this chapter, I will describe, in full detail, all the algorithms and all the theoretical background needed to start the implementation part of the project, as well as some lemmas and proofs about working performance of the procedures described.

\section{Complexity analysis}

I have already mentioned the fact that the Travelling Salesman Problem can not be solved in polynomial time. Now, we would like to see in which class does this problem belong to. In order to do that, we do not need the fully detailed definitions of them because they are not the main point of this dissertation - so I will give a brief and informal definition of complexity classes we take interest in here:

\begin{itemize}

\item {\bf $\mathcal{P}$} - the set of all problems which can be correctly solved in polynomial time.
\item {\bf $\mathcal{NP}$} - the set of all problems whose {\it solutions} can be {\it verified} in polynomial time.
\item {\bf $\mathcal{NP}$-hard} - all the problems which can't be solved in polynomial time. This definition is very informal, but it suffices here. % TODO
\item {\bf $\mathcal{NP}$-complete} - problems which are both in $\mathcal{NP}$ and $\mathcal{NP}$-hard.

\end{itemize}

Now, the Travelling Salesman Problem, defined in the introduction is not $\mathcal{NP}$-complete. This might look a bit strange at first glance, but then, we remember that all $\mathcal{NP}$-complete problems are so-called decision problems - problems deciding whether something is true or not.

\smallskip 

It is not hard to see that the TSP does belong in $\mathcal{NP}$-hard, using my previous $\mathcal{NP}$ complexity class definition. Also, for the formal proof, Karp TODO showed in 1972. that the Hamiltonian cycle problem is $\mathcal{NP}$-complete, which implies the $\mathcal{NP}$-hardness of the TSP. But, surprisingly?, it does not belong to $\mathcal{NP}$. In the verification problem, we would be given a graph and one of cycles from that graph, and we are to judge whether that exact cycle is the shortest one. Note that we can not decide that without actually solving the TSP and knowing what the optimal cycle's cost is, so this can not be done in polynomial time knowing the fact we can not solve TSP in that time. So, this definition of the TSP is not in $\mathcal{NP}$, making it not in $\mathcal{NP}$-complete as well.

\smallskip 

The thing that is a bit strange in the above conclusion is the fact that most of the famous `hard' problems are $\mathcal{NP}$-complete as well. But, all those problems are decision problems, unlike our first TSP definition (and pretty much the only sensible definition of this problem). We can artificially make TSP a decision problem, by formulating it as: given a graph $G$ and a number $p$, decide whether the cost of the shortest cycle in $G$ is less than or equal to the number $p$. This, rephrased TSP, is indeed in $\mathcal{NP}$-complete - the verification is straightforwardly done in polynomial time.

\medskip

Conclusion to all this is that we definitely need and that it is actaully sensible to use some alternate problem solving algorithms than just the exact one - we need to apply some heuristics. 

\section{TSP types}

The TSP, as defined in the beginning of the dissertation, can be solved on any kind of graph. However, a lot of real TSP instances do obey some further criteria on the edge weights. Some of these properties of TSP instances can help us in developing some algorithms which can profit from the fact that the weights are behaving somehow. In this section, I will introduce a couple of terms that will come in handy later.

\smallskip

A TSP instance is called a {\bf Symmetric TSP} instance if and only if the distance matrix is symmetric. So, in the case of a symmetric TSP, for each $u$ and $v$, $w(u \rightarrow v) = w(v \rightarrow u)$. Note that most of the non-artificially generated graphs are symmetric, making this a very natural restriction on edge weights, so in most of the cases we will focus on this type of TSP. 

{\bf Metric TSP} is a subset of symmetric TSP where distance matrix obeys the {\it triangle inequality}. That means, for each triplet of vertices $a$, $b$, $c$ (we can assume they are pairwise connected so we don't have to deal with infinities), $w(a \rightarrow b) <= w(a \rightarrow c) + w(c \rightarrow b)$. It doesn't make sense that the triangle inequality doesn't hold for something which is a road network or similar. Specially, if we can represent our nodes as a points in a Euclidean space and the distances are Euclidean distances between the corresponding points, that graph belongs to a special subset of Metric TSP called the {\bf Euclidean TSP}\footnote{For Euclidean TSP, a PTAS (polynomial-time approximation scheme) algorithm can be applied, which finds the solution that is within a factor of $1 + \frac{1}{c}$ in $\mathcal{O}(n(\log n)^{(\mathcal{O}(c \sqrt{d}))^{d - 1}})$, where d is the dimension of the Euclidean space.}. % TODO ADD CITE

\section{Exact algorithms}

In this section, we will see a few algorithms that solve the Travelling Salesman problem optimally - their output is guaranteed to be correct. As we have shown in the previous section, there can be no polynomial time algorithm belonging to this category. Furthermore, there is no algorithm with running time $\mathcal{O}(1.99999^{n})$, where $n$ is the number of vertices of the graph. That doesn't stop us in comparing different algorithms here, because we can clearly infer that one algorithm is better than the other in certain circumstances.

\subsection{Bruteforce algorithm}

The very first idea that would come up on our minds when taking a look at this problem is that we can try all the possible cycles. Number of cycles in a graph is equal to the number of permutations of the set of vertices. If there are $n$ vertices in the graph, we have $n!$ permutations (and $n!$ candidate solutions). Implementing this naively, we need $\mathcal{O}(n)$ time to calculate the cost for a given cycle, so the naive implementation of this algorithm results in $O(n * n!)$ complexity.

\subsection{Held-Karp algorithm} % nekako nesto citiraj ovde TODO

However, we can do a lot better than the {\it bruteforce algorithm}. We will try to use the dynamic programming approach - use the solution of already solved smaller subproblems in order to solve the bigger subproblem, and repeat this until we come to the whole problem - find the optimal cycle on the whole graph. 

\smallskip

Let us have currently visited $k < n$ vertices, and let those vertices be $v_{1}$, $v_{2}$, $\dots$, $v_{m}$. Note that for the continuation of the algorithm we don't actually need to keep track of the order in which we have traversed all the nodes but $v_{m}$ - the last node currently visited. So, instead of keeping track of $(v_{1}$, $v_{2}$, $\dots$, $v_{m})$, we can keep track of $(\{v_{1}$, $v_{2}$, $\dots$, $v_{m}\}$, $v_{m})$, meaning we can describe the state we are currently in as a pair of two things - a set of all the nodes we have visited and the last node we have visited. Note that this way of bookkeeping saves us a lot of space and time - the different states we can be in like this is $O(2^{n} * n)$, where the first part comes from all the possible subsets of nodes and the second part comes from all the possible ways to choose the second node. This is a large improvement compared to number of states if state were a list of nodes visited so far - that would add up to $O(n!)$.

\smallskip

Speaking about the algorithm, from one state we can transition to $O(n)$ new states, making the complexity of this algorithm $O(n^2 * 2^n)$. This complexity is still exponential, but it is much faster than previous algorithm. Just for the sake of clarity, if we want our algorithm to finish in couple of seconds on an average PC, we can run {\it bruteforce algorithm} for $n$ up to 12, where we can run our dynamic programming algorithm for $n$ up to 28 \footnote{These numbers are based on my implementation of algorithms ran on my PC}.

\subsection{Branch and bound algorithm}

Here, we come to the first algorithm which running time actually depends on the input data. From now on, TODO almost all the algorithms involved will have their correctness and running time impacted by the actual input. Branch and bound algorithms are, in general, basically `smart' bruteforce algorithms in which we don't explore a certain path if we know that there is no chance that we can find an optimal solution continuing along that path. Because this is, after all, a bruteforce algorithm, it's worst case running time has $n!$ as one of the factors, but we hope and that will empirically be shown that this algorithm actually finishes much faster than the bruteforce algorithms in most cases. Also, which might be not completely intuitive, this algorithms is also better than the Held-Karp algorithm on average, being able to solve some TSP instances with $n = 40$ in couple of seconds.

\smallskip

First, what we want to do is to compute some lower bound of the cheapest cycle, and ideally we would like to compute that fast. For the sake of clarity, we can say that our graph is complete - if there is no edge between two vertices, we can assume that the edge exists, having value $\infty$. Now, we can take a look at $n \times n$ distance matrix $M$ of the graph, where $M_{i, j} = w(i \rightarrow j)$. Also, the assumption is that the graph doesn't have any self-loops, so $M_{i, i} = \infty$ for every $i$.

\smallskip

Let's consider one row of the matrix. If we have picked $v$-th row, that row is consisted of weights of all the outgoing edges from vertex $v$. In the optimal solution, at least one edge has to be a edge that is outgoing from $v$. So, if $m_{i} = \min(M_{i, 1}, M_{i, 2}, \dots, M_{i, n})$, we are sure that the edge going out from $v$ is going to contribute to the solution by some value which is greater or equal to $m_{i}$. Hence, we can conclude that the cost of the optimal solution must be at least $min\_cost = \sum_{i = 1}^{n} M_{i}$. After having that calculated, we can deduct $m_{i}$ from $M_{i, .}$ and get $M'$. It's easy to see that $min\_cost + tsp\_cost(M') = tsp\_cost(M)$. 

\smallskip

We can perform the same procedure on the colums of the new matrix (repeating the same argument for incoming edges of each vertex), and adding the cost we got here to the row-obtained cost. That value is definitely a lower bound for the solution, and hypothetically it can be equal to the optimal TSP cost, in case there is a cycle with value 0 in the newly formed distance matrix. 

\smallskip

Finally, the idea of the branch and bound algorithm here is that we will discard all the states where $current\_cost + tsp\_approximation(current\_state) >= best\_solution\_so\_far$. This termination condition is not the strongest one we can find in polynomial time, but it works quite good. In evaluation chapter, we will see some numbers about number of explored states using this algorithm.

\section{Approximation algorithms}

We will not produce the optimal solutions here, but we will have some guarantees about the value we obtain using these algorithms. If we don't require our solution to be fully optimal, but we just don't want to be too wrong, approximation algorithms can guarantee the solution we want really fast.

\smallskip

We should note here that it is not possible to polynomially approximate the TSP as good as we want. Determining how good can we approximate the problem is still a major open problem in the field of computer science. A real number $r$ is said the be an inapproximability bound for an optimization problem $\mathbb{P}$ if it is proved that a problem having to find a solution that is better than $r$ times the optimal solution for $\mathbb{P}$ is $\mathcal{NP}$-hard. A paper TODO CITAT released in 2013 improves the inapproximability bounds to $185/184$ for the symmetric TSP, and $117/116$ for the asymmetric TSP., though there is no proof that we can't improve the bounds further.

\subsection{2-approximation algorithm}

If we try to think of a something in the graph which is impacted by all the nodes, have it's cost minimal and is easy to calculate, we remember that there is something called a {\bf minimum spanning tree}. That is a subgraph of the graph which is a tree\footnote{A tree graph is a connected acyclic graph. A tree with $n$ nodes has $n - 1$ edges.}, with vertex set equal to the vertex set of the original graph, with minimal sum of the weights of all the edges.

\smallskip

Our 2-approximation algorithm runs as follows: 

\begin{enumerate}

\item Pick a start vertex {\it start} - it doesn't matter how we choose this one.
\item Construct a MST with 1 as root, using, for example, Prim's algorithm
\item List all the vertices according to preorder walk, and add {\it start} to the end

\end{enumerate}

This gaves us a cycle of all $n$ vertices.

\smallskip

We will first state and prove one lemma here.

\begin{lemma}

\label{mstlemma}
The cost of a MST is less than or equal to the cost of a minimum cycle.

\end{lemma}

\begin{proof}

This is pretty straightforward, because the cost of a TSP is less than or equal to a cost of the TSP without a single edge, which is a spanning tree as well, so it's cost cannot be less to a cost of a minimum spanning tree.

\end{proof}

\begin{theorem}[2-approx theorem]

For metric TSP, the cost of the cycle obtained above is at most 2 times more expensive that the optimal cycle. \\

\end{theorem}

\begin{proof}

Let's consider a full preorder walk  of the graph - basically a preorder walk, but we list the vertex again when we return to it. For example, in the tree on figure 1, the preorder walk with $start = 1$ would be $[1, 2, 3, 2, 4, 2, 1, 5, 1]$. In order to prove this, we will separately show three facts:

\begin{itemize}

\item The cost of a full preorder walk is at most two times larger than the cost of MST - every edge of the MST is visited at most twice.

\item The cost of a cycle generated by this algorithm is not greater than the cost of a full preorder walk - this is easily shown using the triangle inequality property\footnote{This is the part that requires the TSP to be metric} of the TSP, because two adjacent edges are constantly being replaced by one connecting non-adjacent ends when transitioning from a full preorder walk to the cycle above.

\end{itemize}

Using these two facts, as well as \hyperref[mstlemma]{MST lemma} TODO we conclude that the theorem 1 holds.

\end{proof}

\subsection{Christofides algorithm} %TODO citat

This algorithm gives us a 1.5 approximation to the metric TSP. It was developed by Nicos Christofides in 1976., and as of 2019 it is the best approximation ratio that has been proven for the TSP on general metric spaces, although there are some better approximation for some special cases (e.g. Euclidean TSP).

The algorithm runs as follows:

\begin{itemize}

\item Let $G$ be the starting graph. Find a MST of $G$, call it $T$.
\item Create a subraph of $G$, $O$, consisting of vertices with odd degree in $T$ and all the edges between them.
\item Let $PM$ be the minimum-weight perfect matching in $O$. Let $T'$ be the graph with all the nodes and all the edges from both $T$ and $PM$.
\item Construct any Eulerian cycle on $T'$, and reduce it to a Hamiltonian circuit by skipping repeated nodes. Return that Hamiltonian cycle.

\end{itemize}

\begin{theorem}[Christofides theorem]

For metric TSP, the cost of the cycle obtained above is at most 1.5 times larger than the cost of the optimal cycle.

\end{theorem}

\begin{proof}

Christofides proof here.

\end{proof}

\section{Improvement algorithms}

The general idea here is that we start from any solution, and try to iteratively improve the solution until we converge or until we are satisfied with the cycle found. There are no guarantees of any kind for iterative improvement algorithms, but we will empirically validate later on that they do perform better than some algorithms with stronger mathematical background.

\subsection{k-opt algorithm}

The idea behind k-opt heuristic, for given $k$, is as follows: until convergence or until some other stopping point, cut the cycle in $k$ chains, and try to rearrange these chains to obtain the least expensive cycle. Note that there is a significant tradeoff between running time for one iteration (which is $\mathcal{O(nk!)}$) and the improvement we can get (more small parts we cut it into, we have greater probability of being able to improve it). In a degenerate case, picking $k = n$ would yield a optimal solution after one iteration, but that iteration would take the same amount of time bruteforce does. 

\smallskip

It is empirically validated that picking $k >= 4$ is worse than picking $k <= 3$. In my implementation, I have coded 2-opt and 3-opt separately TODO, and shown that 3-opt performs better than 2-opt.

\subsection{Lin-Kernighan heuristic algorithm}

The problem with k-opt is that we are setting the value of $k$ in advance. Lin-Kernighan algorithms is basically a generalization of k-opt, at each iteration trying to pick the best value of $k$ to do the interchange. It is one of the most successful TSP heuristic today, and it is a heuristic giving the best results in my implementation.

The procedure runs as follows:

LKH here

\section{Heuristic algorithms}

The TSP is a really famous problem. Having that in mind, it's sensible that a lot of heuristics, some better than the other, were tried in order to get close to the optimal solution. In this section, we will discuss some of the attempts tried, and later we will show how do they compare to each other.

\subsection{Random algorithm}

This algorithm isn't really something one would try in order to solve the TSP, but it is included in the project so we would get an idea of how good other algorithms really are. We will just take the best solution out of some predefined number of random cycles.

\subsection{NN (greedy) - algorithm}

The nearest neighbour greedy algorithm is probably the first one to come up to someone's mind when tackling this problem. Also, it doesn't require knowing anything about the whole graph when making the decision, only about the surroundings, so it is not strange that an actual salesman from the problem name would use it if necessary. 

\smallskip

When in a certain node, the algorithms chooses the edge with minimum cost leading to some, so far unvisited, node and traverses to there. After visiting all the nodes, it returns to the node where the path has started. It is not hard to see that the solution depends on the choice of the starting node, hence we can start from all the nodes and take the minimum solution which improves the solution with only a factor of $n$. 

\smallskip

This algorithm can go horribly wrong, and we can construct a case where the solution obtained this way is arbitrarily times longer than the optimal solution. Still, it performs surprisingly fine on average.

\subsection{Insertion heuristics}

With all the insertion heuristics, the recipe is the same: starting from a single node with a self-loop (an one node cycle), we are searching for an optimal node and an optimal spot. Having found that, we insert the node at a proposed place, and keep going until we insert all the nodes, resulting in a solution. Also, starting from different nodes might lead to a different solution, so we can try all the possible starts.

\smallskip

There are multiple variations here, and the ones I have implemented can be found below:

\subsubsection{Cheapest insertion heuristic}

\subsubsection{Farthest insertion heuristic}

\subsubsection{Nearest insertion heuristic}

\subsubsection{Random insertion heuristic}

\subsubsection{Convex-hull insertion heuristic}

\section{Optimisation algorithms}

\subsection{Ant colony optimisation algorithm}

\cleardoublepage
\chapter{Implementation}



\cleardoublepage
\chapter{Evaluation}

\cleardoublepage
\chapter{Conclusion}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\nocite*{\bibliography{refs}}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\cleardoublepage


\cleardoublepage

\chapter{Project Proposal}

\input{temp_propbody}

\end{document}
